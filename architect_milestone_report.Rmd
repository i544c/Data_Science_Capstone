---
title: "Peer-graded Assignment: Milestone Report"
author: "Author: Isaac G Veras"
date: "October 5, 2023"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

![](https://media.licdn.com/dms/image/C4D12AQGD_su1k14bYA/article-cover_image-shrink_600_2000/0/1583217311227?e=2147483647&v=beta&t=s_7cvkGjyfNTp2x6mnsiPFUfbPhWyvnMIavE_na62bE)

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Instructions

The goal of this project is to publish on <a href="http://rpubs.com/">R Pubs</a> a report that explains the exploratory analysis and its goals for the application and algorithm. This document should be concise and explain only the key features of the data you identified and briefly summarize the plans for creating the Shiny prediction algorithm and application in a way that is understandable to a non-Data Scientist manager. Present tables and graphs to illustrate important summaries of the data set. The motivation for this project, therefore, is:

1.  Demonstrate that the data download was imported and loaded successfully.

2.  Create a basic summary statistics report on the datasets.

3.  Report all interesting discoveries to date.

4.  Get feedback on plans to create a Shiny prediction algorithm and app.

**Libraries to be loaded for Data Analysis**

```{r install packages, message=FALSE, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        pacman,
        knitr,
        tidyverse,
        NLP,
        openNLP,
        qdapDictionaries,
        qdapRegex,
        qdapTools,
        slam,
        tools,
        RWeka,
        ngram,
        stringr,
        RColorBrewer,
        SnowballC,
        wordcloud,
        wordcloud2
)

```

**Final Product**

The final product of the project will be an algorithm that predicts the next word in text provided with inputs from the test dataset, similar to the text prediction functions found in today's modern smartphones.

# Data Set

HC Corpora's `Swiftkey` dataset is comprised of output from several News, Blogs and Twitter sites. The dataset contains 3 files in four languages (Russian, Finnish, German and English). This project will focus on English (en) language datasets:

1.  en_US.blogs.txt
2.  en_US.twitter.txt
3.  en_US.news.txt

**`NOTE:`** `and these will be referred to as "Blogs", "Twitter" and "News" in the remainder of this report`

**Tasks performed on data**

1.  Explore the training dataset;
2.  Profanity filtering - removal of inappropriate terms;;
3.  Tokenization -- identifying appropriate tokens such as words, punctuation and numbers

## Importing the Data:

The training datasets for this project were downloaded via this <a href="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip">link</a>

```{r, eval=TRUE, cache=FALSE}
library(dplyr)
library(kableExtra)
 
blogsC        <- file("./swiftkeys/en_US.blogs.txt"  , "r")
newsC         <- file("./swiftkeys/en_US.news.txt"   , "r")
twitterC      <- file("./swiftkeys/en_US.twitter.txt", "r")

blogs         <- readLines(blogsC,   n = -1, encoding = "UTF-8")
news          <- readLines(newsC,    n = -1, encoding = "UTF-8")
twitter       <- readLines(twitterC, n = -1, encoding = "UTF-8")

close(blogsC)
close(newsC)
close(twitterC)

nCharBlogs    <- sum(nchar(blogs))
nCharNews     <- sum(nchar(news))
nCharTwitter  <- sum(nchar(twitter))

lenBlogs      <- length(blogs)
lenNews       <- length(news)
lentwitter    <- length(twitter)

nWordsBlogs   <- sum(sapply(strsplit(blogs, " "), length))
nWordsNews    <- sum(sapply(strsplit(news, " "), length))
nWordsTwitter <- sum(sapply(strsplit(twitter, " "), length))

outputTable   <- data.frame(
    c("blogs", "news", "twitter"),
    c(nCharBlogs, nCharNews, nCharTwitter),
    c(lenBlogs, lenNews, lentwitter),
    c(nWordsBlogs, nWordsNews, nWordsTwitter)
)
colnames(outputTable) <- c("FileType", "Characters", "Lines", "Words")

rm(blogs, news, twitter)

```

### SwiftKeys (EN-US)

```{r, echo=FALSE}
kable(head(outputTable)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

**Steps taken:**

-   Code to download the file and unzip is hidden and its with the assumption that files are available in the current working directory;
-   Open the file connection;
-   Close connection;
-   Compute n°. of characters;
-   Compute n°. of lines;
-   Compute n°. of words;
-   Build a Dataset Summary output Table;
-   Release object memory;
-   Summary of Data Analysis.

## Exploratory Analysis of Data

Since the training data set size is huge, a sample of each file is extracted and explored for further study

```{r, echo=FALSE}
library(dplyr)

linesToExtract <- 10000 

blogsC         <- file("./swiftkeys/en_US.blogs.txt"  , "r")
newsC          <- file("./swiftkeys/en_US.news.txt"   , "r")
twitterC       <- file("./swiftkeys/en_US.twitter.txt", "r")

blogsS         <- readLines(blogsC,   n=linesToExtract,  encoding = "UTF-8")
newsS          <- readLines(newsC,    n= linesToExtract, encoding = "UTF-8")
twitterS       <- readLines(twitterC, n=linesToExtract,  encoding = "UTF-8")

close(blogsC)
close(newsC)
close(twitterC)

nCharBlogs     <- sum(nchar(blogsS))
nCharNews      <- sum(nchar(newsS))
nCharTwitter   <- sum(nchar(twitterS))

lenBlogs       <- length(blogsS)
lenNews        <- length(newsS)
lentwitter     <- length(twitterS)

nWordsBlogs    <- sum(sapply(strsplit(blogsS, " "), length))
nWordsNews     <- sum(sapply(strsplit(newsS," "), length))
nWordsTwitter  <- sum(sapply(strsplit(twitterS," "), length))

sampleOutputTable <- data.frame(
        c("blogsSample", "newsSample", "twitterSample"),
        c(nCharBlogs, nCharNews, nCharTwitter),
        c(lenBlogs, lenNews, lentwitter),
        c(nWordsBlogs, nWordsNews, nWordsTwitter)
)
colnames(sampleOutputTable) <- c("FileType", "Characters", "Lines", "Words")

```

### Sample Data:

```{r, echo=FALSE}
kable(head(sampleOutputTable)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

**Steps taken:**

-   Set the line count information for extraction;
-   Open the file connection;
-   Close the file connection;
-   Compute n°. of characters;
-   Compute n°. of lines;
-   Compute n°. of words;
-   Build a Dataset Summary output Table

# Data Cleansing & Corpora building

The text sample extracted from each of the files is transformed step-by-step for Predictive Model bulding

```{r, include=FALSE}
library(tm)
library(qdap)

beforeDF      <- data.frame(text=c(blogsS, newsS, twitterS))
beforeCorpora <- VCorpus(VectorSource(beforeDF))
sampleTxtBP   <- strwrap(head(beforeCorpora[[1]]$content,5))

twitterS      <- gsub ("(RT|via)((?:\\b\\W*@\\w+)+)", "", twitterS)
twitterS      <- gsub("@\\w+", "",twitterS)

sampleData    <- c(blogsS, newsS, twitterS)
sampleData    <- replace_abbreviation(sampleData)

sampleDF      <- data.frame(text = sampleData)

sampleCorpora <- VCorpus(VectorSource(sampleDF))

rm(blogsS, newsS, twitterS)
rm(beforeDF, beforeCorpora,sampleDF,sampleData)

sampleCorpora <- tm_map(sampleCorpora,content_transformer(tolower))

removeOnlineJunk <- function(x) {
    x <- gsub("[^ ]{1,}@[^ ]{1,}"," ",x)
    x <- gsub(" @[^ ]{1,}"," ",x)
    x <- gsub("#[^ ]{1,}"," ",x) 
    x <- gsub("[^ ]{1,}://[^ ]{1,}"," ",x)
}
sampleCorpora <- tm_map(sampleCorpora,content_transformer(removeOnlineJunk))

removeSymbols <- function(x){
    x <- gsub("[`'']","'",x)
    x <- gsub("[^a-z']"," ",x)
    x <- gsub("'{2,}"," '",x)
    x <- gsub("' "," ",x)
    x <- gsub(" '"," ",x)
    x <- gsub(","," ",x)
    x <- gsub("^'","",x)
    x <- gsub("'$","",x)
    x
}
sampleCorpora <- tm_map(sampleCorpora,content_transformer(removeSymbols))
sampleCorpora <- tm_map(sampleCorpora, content_transformer(removePunctuation))
sampleCorpora <- tm_map(sampleCorpora, content_transformer(removeNumbers))

profanityFileName <- "profanity.txt"
if (!file.exists(profanityFileName)) 
download.file(
  url      =  "http://www.cs.cmu.edu/~biglou/resources/bad-words.txt",
  destfile = profanityFileName
  )
profC         <- file("./profanity.txt", "r")
profanityL    <- readLines(profC, n=-1,encoding = "UTF-8")
close(profC)

sampleCorpora <- tm_map(sampleCorpora, removeWords, profanityL)
sampleCorpora <- tm_map(sampleCorpora,content_transformer(stripWhitespace))

sampleTxtAP   <- strwrap(head(sampleCorpora[[1]]$content,5))
```

**Steps taken:**

**`NOTE`** `Optional steps - collate Sample Text Data and create a reduced raw data file before processing`

```         
  beforeData <- c(blogsS, newsS, twitterS)
```

-   Remove retweets from Twitter Data Sample;
-   Remove @people names from Twitter;
-   Collate text data from different file samples;
-   Replace abbreviation so that the sentences are not split at incorrect places;

**`NOTE:`** `Optional steps - convert paragraphs to sentences`

```         
  endNotations <- c("?", ".", ",","!", "|", ":", "\n", "\r\n")<
  sampleData <- sent_detect(
                            sampleData, 
                            endmarks   = endNotations, 
                            rm.bracket = FALSE
                            )
```

-   Collate Sample Text Data and create a reduced raw data file;
-   Create Text Corpus for processing;
-   Release object memory;
-   Convert the text to lower case;
-   Remove URL from Corpora, symbols;
-   Replace emails and such but space, websites and file systems;
-   Edit out most non-alphabetical character, text must be lower case first;
-   Remove the punctuations after trimming leading & trailing white spaces;
-   Remove numbers from the text;
-   Build profane word list;
-   Remove profane words from the corpora;
-   Remove all the white space that was created by the removals;
-   Sample Text Data after processing.

**Please find below the first few lines of text data before and after processing in the Corpora:**

```{r, cache=FALSE}
## Before Text Processing
sampleTxtBP

## After Text Processing 
sampleTxtAP

```

```{r, echo=FALSE}
# Release object memory 
rm(sampleTxtBP, sampleTxtAP)
```

# Tokenization of text - Creation of N-grams

Perform Tokenization, and thus obtain one (uni-), two (bi-), three (tri-), and four (tetra-) word combinations that appear frequently in the Text Corpus.

```{r, cache=FALSE}
library(dplyr)

strCorpus <- concatenate(lapply(sampleCorpora,"[",1))
ng1 <- ngram(strCorpus, n=1)
ng2 <- ngram(strCorpus, n=2)
ng3 <- ngram(strCorpus, n=3)
ng4 <- ngram(strCorpus, n=4)

```

# Inspect first few entries in N-grams generated {.tabset .tabset-fade}

## NG1

```{r, echo=FALSE}
kable(head(get.phrasetable(ng1), 10)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## NG2

```{r, echo=FALSE}
kable(head(get.phrasetable(ng2), 10)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## NG3

```{r, echo=FALSE}
kable(head(get.phrasetable(ng3), 10)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## NG4

```{r, echo=FALSE}
kable(head(get.phrasetable(ng4), 10)) %>% 
  kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```

## Make new sentence:

To make new sentence using the n-grams generated, try babble(`ng=ng2, genlen=15, seed= 123445`) which would return a random formed 15 word length sentence.
```{r, echo=TRUE}
babble(ng=ng2, genlen=15, seed= 12112344)
```

# Visual Inspection of tokenized words

Using the corpus of documents, we now construct a Document Term Matrix (DTM). This object is a simple triplet matrix structure (efficient for storing large sparse matrices), that has each document as a row and each n-gram (or term) as a column.

-   Build Term document matrix with single tokenizer and words smaller than 3 characters are omitted:

```{r, cache=FALSE}
sampleTDM <- tm::TermDocumentMatrix(sampleCorpora, control = list(wordLengths = c(3,Inf)))

#Put word count from TDM to data frame 
sampleFreqWords <- data.frame(word = sampleTDM$dimnames$Terms, frequency = sampleTDM$v)

# Reorder the word list in descending order 
sampleFreqWords <- plyr::arrange(sampleFreqWords, -frequency)

# Build Most frequent terms 
n <- 25L # variable to set top n words
# isolate top n words by decreasing frequency
sampleFreqWords.top <- sampleFreqWords[1:n, ]
# reorder levels so charts plot in order of frequency
sampleFreqWords.top$word <- reorder(sampleFreqWords.top$word, sampleFreqWords.top$frequency)
```

# Term analysis {.tabset .tabset-fade}

## Frequent Terms

```{r, echo=FALSE, warning=FALSE, message=FALSE}
g.sampleFreqWords.top <- ggplot(sampleFreqWords.top, aes(x = word, y = frequency))
g.sampleFreqWords.top <- g.sampleFreqWords.top + geom_bar(stat = "identity") + coord_flip() + labs(title = "Most Frequent Terms based on Unigrams")
g.sampleFreqWords.top
```

## Wordcloud

```{r, echo=FALSE}
pal <- brewer.pal(6,"Dark2")
pal <- pal[-(1)]
wordcloud(sampleFreqWords.top$word, sampleFreqWords.top$frequency,c(8,.3),2,,TRUE,TRUE,.15,pal)
```

**Next Steps Forward - Prediction Algorithm**

Moving forward, the project goal is to develop a natural language prediction algorithm and app. For example, if a user were to type, "I want to go to the .", the app would suggest the three most likely words that would replace ".".

# N-gram Dictionary

While the word analysis performed in this document is helpful for initial exploration, the data analyst will need to construct a dictionary of bigrams, trigrams, and tetra-grams, collectively called n-grams. Bigrams are two word phrases, trigrams are three word phrases, and four-grams are four word phrases.\

Here is an example of trigrams from the randomly sampled corpus. Recall that stop words had been removed so the phrases may look choppy. In the final dictionary, stop phrases and words of any length will be maintained.

```{r, echo=FALSE}
library(dplyr)

# tokenizer functions 
UniGramTokenizer <- function(corpus) {
    NGramTokenizer(corpus, Weka_control(min = 1, max = 1))
}

BiGramTokenizer <- function(corpus) {
    NGramTokenizer(corpus, Weka_control(min = 2, max = 2))
}

TriGramTokenizer <- function(corpus) {
    NGramTokenizer(corpus, Weka_control(min = 3, max = 3))
}

TetraGramTokenizer <- function(corpus) {
    NGramTokenizer(corpus, Weka_control(min = 4, max = 4))
}

##sample Trigram analysis - Most frequent terms 
trigram.sampleTDM <- tm::TermDocumentMatrix(sampleCorpora, control = list(tokenize = TriGramTokenizer))
# put into data frame
freq.trigram <- data.frame(word = trigram.sampleTDM$dimnames$Terms, frequency = trigram.sampleTDM$v)
# reorder by descending frequency
freq.trigram <- plyr::arrange(freq.trigram, -frequency)
```

# Trigrams {.tabset .tabset-fade}

## Frequent Terms Trigrams

```{r, echo=FALSE}
## Extract most frequent tri-grams 
kable(head(freq.trigram,10)) %>%
        kable_styling(full_width = FALSE)
```

## Wordcloud Trigrams

```{r, echo=FALSE}
pal <- brewer.pal(6,"Dark2")
pal <- pal[-(1)]
wordcloud(freq.trigram$word, freq.trigram$frequency,c(8,.3),2,50,TRUE,TRUE,.15,pal)
```

# Predicting from N-grams

Each n-gram will be split, separating the last word from the previous words in the n-gram.

-   bigrams will become unigram/unigram pairs
-   trigrams will become bigram/unigram pairs
-   four-grams will become trigram/unigram pairs

For each pair, the three most frequent occurrences will be stored in the dictionary. Here are the three most frequent trigrams for a bigram of "cant wait" from the randomly sampled corpus. These eleven trigrams would be split into bigram/unigram pairs and stored in the sample dictionary. Dictionaries will be built for the whole data set

```{r, cache=FALSE}
freq.trigram %>% filter(str_detect(freq.trigram$word,"^cant wait"))
```

## Application Logic

After the dictionaries have been established, an app will be developed allowing the user to enter text. The app will suggest the three most likely words to come next in the text for the text type, based on these rules.

-   If the supplied text is greater than 2 words, take the last three words of the text and search the trigram/unigram pairs.
-   If the supplied text is 2 words, take the two words and search the bigram/unigram pairs.
-   If the supplied text is 1 word, search for that word in the unigram/unigram pairs.
-   Suggest the three most frequent unigrams from the n-gram/unigram pair for either 1, 2, or 3 above.

------------------------------------------------------------------------

![](https://i0.wp.com/www.globalemancipation.ngo/wp-content/uploads/2017/09/github-logo.png?ssl=1){width="10%"} [i544c](https://github.com/i544c/Data_Science_Capstone)
